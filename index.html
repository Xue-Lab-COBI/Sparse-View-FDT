<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo">
  <meta name="keywords" content="Compressive sensing, Deep tissue imaging">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
 
    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon2.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<!--   <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .column.content img {
      width: 100%;
      height: auto;
    }
    .caption {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text .section-title {
      font-weight: bold;
      font-style: italic;
    }
    .scientific-notation {
      font-family: 'Times New Roman', Times, serif;
      font-style: italic;
    }
    .superscript {
      vertical-align: super;
      font-size: smaller;
    }
  </style>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Renzhi He</a>,</span>
            <span class="author-block">
              <a href="">Yucheng Li</a>,</span>
            <span class="author-block">
              <a href="">Brianna Urbina</a>,</span>
            <span class="author-block">
              <a href="">Jiandi Wan</a>,</span>
            <span class="author-block">
              <a href="https://cobi.ucdavis.edu/people">Yi Xue*</a>,
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">University of California, Davis,</span>
<!--             <span class="author-block"><sup>2</sup>Google Research</span> -->
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="http://arxiv.org/abs/2407.16657"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="http://arxiv.org/abs/2407.16657"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/cubhe/fdt_code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">



  
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Two-photon microscopy is a powerful tool for in vivo imaging, but its imaging depth is typically limited to a few hundred microns due to tissue scattering, even with existing scattering correction techniques. Moreover, most active scattering correction methods are restricted to small regions by the optical memory effect. Here, we introduce compressive Fourier-domain intensity coupling for scattering correction (C-FOCUS), an active scattering correction approach that integrates Fourier-domain intensity modulation with compressive sensing for two-photon microscopy. C-FOCUS enhances fluorescence intensity by over 20-fold in vivo, accelerates scattering correction by 5-fold compared to previous approaches, and enables multipatch correction across fields of view spanning hundreds of microns beyond the memory effect range. Using C-FOCUS, we demonstrate high-resolution imaging of YFP-labeled neurons and FITC-labeled blood vessels at depths exceeding 900 &mu m in the intact mouse brain in vivo. Furthermore, we achieve transcranial imaging of YFP-labeled dendritic structures through the intact adult mouse skull. C-FOCUS provides a broadly applicable strategy for rapid, deep-tissue optical imaging in the living brain.
          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Overview</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/Figure1.jpg" alt="Matting Example" style="width: 100%; height: auto;">
            <p class="caption">
              <strong>C-FOCUS enables two-photon imaging of YFP-labeled pyramidal neurons up to 940 $\mu m$ below the dura in the somatosensory cortex using intensity-based scattering correction with compressive sensing.</strong>
            </p>
            <p>
              <span class="caption">a</span>, Optical schematic of the C-FOCUS system, based on the 2P-FOCUS framework. 
              <span class="caption">b</span>, an uncorrected image is first acquired and segmented into subregions, each containing a local intensity peak that serves as a correction target.
              <span class="caption">c</span>, Correction masks for the subregions are measured and calculated sequentially using compressive sensing.
              <span class="caption">d</span>, In vivo imaging of YFP-labeled pyramidal neurons in the somatosensory cortex of a Thy1-YFP-H mouse through a cranial window using 1035 nm excitation, with and without scattering correction. With C-FOCUS, fluorescence intensity is enhanced fivefold, enabling imaging to a depth of 800 $\mu$m (8.5 EAL).
            </p>

          </div>
        </div>
      </div>
      
      <style>
        .column.content img {
          width: 100%;
          height: auto;
        }
      </style>
    </div>

    <!--/ Matting. -->



<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Results</h2>

    <div class="content has-text-justified">
      <p>
        We verify the validation of our model as well as each component in the
        model under the simulation 'ucdavis' data. Then, we applied the FDT to experimental
        data of thin (MDCK) and thick (3D muscle tube) sample to show the effectiveness of our model for variety of
        structure and z-section ability. 
      </p>
      <div class="related-work">
        <h3 class="title is-4">UCDavis</h3>
        <div class="work-item">
          <a href="">
            <img src="./static/images/fig_ucd3.jpg" alt="Progressive Encoding for Neural Optimization">
          </a>
          <p class="caption">
            <strong>Reconstruction of the 3D RI of a simulated "UCDavis" pattern.</strong>
          </p>
          <p>
            <span class="caption">a</span>, Reconstructed 3D RI distribution of the "UCDavis" pattern in 3D from 400 fluorescence images. 
            <span class="caption">b</span>, A representative ground-truth (GT) image, generated using the multi-slice model with the ground-truth 3D RI, where the RI of the letters is 1.38 and the background is 1.33. 
            <span class="caption">c</span>, The predicted fluorescence image under the same illumination as <span class="caption">b</span> with the reconstructed 3D RI. 
            <span class="caption">d</span>, Zoomed-in views of the regions within the orange and green boxes in <span class="caption">b</span> and <span class="caption">c</span>, respectively. The SSIM and PSNR between the ground-truth and predicted images are 0.9994 and 56.9062, respectively. 
            <span class="caption">e</span>, Results of self-calibration of the positions of fluorescent sources at 50, 100, and 150 iterations. The blue circle indicates the irradiated region from the ground-truth position of the excited fluorophore, and the white dashed line indicates the irradiated region from the predicted position of the fluorophore. 
            <span class="caption">f</span>, The plot on the right shows the MSE loss between the self-calibrated and ground-truth fluorophore positions, converging to 0.001 within 500 iterations, indicating successful self-calibration of the positions of fluorescent sources. 
            <span class="caption">g</span>, Comparison of the ground-truth RI (top row) and the predicted RI (bottom row) on each <i>z</i>-plane, as indicated by the axis below. 
            <span class="caption">h</span>, Effect of the coarse-to-fine structure on reconstruction results on two different <i>z</i>-planes (top row, <i>z</i> = 65 μm; bottom row, <i>z</i> = 53 μm). The first three columns show results of the coarse-to-fine structure at different iterations (100, 200, and 300) with progressively increasing sampling grid at 128 × 128, 256 × 256, and 512 × 512 pixels. The last column shows the results after 300 iterations without the coarse-to-fine structure at a sampling grid of 512 × 512 pixels. Comparing the third column and the fourth column, the coarse-to-fine structure mitigates crosstalk between <i>z</i>-planes and reconstructs the missing low-frequency signals.
          </p>
        </div>
        
        
        <div class="work-item">
          <h3 class="title is-4">MDCK</h3>
          <a href="">
            <img src="./static/images/fig_mdck3.jpg" alt="D-NeRF">
          </a>
<!--           <p class="caption">3D RI Reconstruction of a thin sample of live MDCK cells.</p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI distribution within a volume of 358.4 x 358.4 x 60 µm<span class="superscript">3</span> . 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images, with a highlighted region of interest. 
            <span class="caption">c</span>, Detailed views of the highlighted region: measured image (top), predicted image (middle), and error map (bottom). 
            <span class="caption">d-f</span>, Cross-sectional views of the RI difference  distribution along different planes (xy, xz, yz), illustrating spatial variations in RI. 
            <span class="caption">g</span>, Quantitative analysis of RI variation of the red box in <span class="caption">d</span>, showing RI along the X-axis (top) and Z-axis (bottom). 
            <span class="caption">h</span>, Sequence of images highlighting the appearance and disappearance of specific features within the region along the Z-axis.
          </p> -->
          </a>
          <p class="caption">
            <strong>3D RI reconstruction of a thin layer of live MDCK Cells sample.</strong>
          </p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI distribution of MDCK cells within a 358.4 × 358.4 × 44 μm<sup>3</sup> volume. The RI of the cells ranges between 1.33 to 1.36. 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images. 
            <span class="caption">c</span>, Zoomed-in views of the highlighted region: the measured image (top), the predicted image (middle), and the error map between the measured and predicted images (bottom). 
            <span class="caption">d</span>, Schematic diagram of the optical setup of FDT. Fluorescence is excited by scanning a focus with a spatial light modulator (SLM), and diffracted fluorescence images are captured in reflection mode using a camera. 
            <span class="caption">e-g</span>, Cross-sectional views of the RI distribution of MDCK cells on three representative planes that are 12.5 μm apart, showing optical sectioning ability and high 3D resolution. 
            <span class="caption">h</span>, Zoomed-in view of the image <i>z</i>-stack of cells in the highlighted region in <span class="caption">e</span>. The <i>z</i>-position of each image is labeled on the <i>z</i> axis below the images. The images again highlight the optical sectioning and high resolution of FDT.
          </p>
        </div>
        <div class="columns is-centered">

        <!-- Visual Effects. -->
        <div class="column">
          <div class="content">
            <h3 class="title is-4">2D view</h3>
  <!--           <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p> -->
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ri_mdck_z~1.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Visual Effects. -->
  
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">3D view</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="dollyzoom" autoplay controls muted loop playsinline height="95%">
                <source src="./static/videos/ri_mdck_3d720.mp4"
                        type="video/mp4">
              </video>
            </div>
  
          </div>
        </div>
      </div>




        <p>
        </p>
        <p>
        </p>
        <div class="work-item">
          <h3 class="title is-4">3D tube</h3>
          <a href="">
            <img src="./static/images/fig_tube3.jpg" alt="NR-NeRF">
          </a>
          <p class="caption">
            <strong>3D RI reconstruction of a 3D cultured bovine myotube.</strong>
          </p>
          <p>
            <span class="caption">a</span>, 3D visualization of the RI of 3D cultured bovine myotube within a volume of 530 × 530 × 300 μm<sup>3</sup>. 
            <span class="caption">b</span>, Comparison of the measured (top) and predicted (bottom) images. 
            <span class="caption">c-d</span>, Cross-sectional views of the reconstructed RI on two representative planes, showing high resolution and optical sectioning ability. 
            <span class="caption">e</span>, Zoomed-in details of the highlighted regions in <span class="caption">d</span>, labeled by corresponding colors, showing different morphologies of the 3D cultured bovine myotube during proliferation and differentiation. The results indicate that FDT can accurately reconstruct various structures across a wide range of spatial frequencies. See the video in the supplementary material for a better 3D visualization.
          </p>
        </div>

        <div class="columns is-centered">
        <div class="column">
          <div class="content">
            <h3 class="title is-4">Axial Rotational Perspective</h3>
  <!--           <p>
              Using <i>nerfies</i> you can create fun visual effects. This Dolly zoom effect
              would be impossible without nerfies since it would require going through a wall.
            </p> -->
            <video id="dollyzoom" autoplay controls muted loop playsinline height="100% style="margin-top: 20px;">
              <source src="./static/videos/3dtube0721_11.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        <!--/ Visual Effects. -->
  
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">Radial Rotational Perspective</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="matting-video" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/3dtube0721_2.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        </div>
        
        <div class="columns is-centered">
        <!-- Matting. -->
        <div class="column">
          <div class="content">
          <h3 class="title is-4">Radial Rotational Perspective</h3>
          <div class="columns is-centered">
  <!--           <div class="column content"> -->
  <!--             <p>
                As a byproduct of our method, we can also solve the matting problem by ignoring
                samples that fall outside of a bounding box during rendering.
              </p> -->
              <video id="matting-video" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/ri_3dtube_z~2.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
        </div>
        
      </div>



        
      </div>
    </div>
  </div>
</div>

<style>
  .related-work {
    display: flex;
    flex-direction: column;
    align-items: center;
  }
  .work-item {
    text-align: center;
    margin-bottom: 20px;
    width: 100%;
  }
  .work-item img {
    display: block;
    margin: 0 auto;
    width: 100%;
    height: auto;
  }
  .caption {
    font-weight: bold;
    margin-top: 10px;
  }
</style>
</div>

<div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      
    
    <!-- Concurrent Work. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>

    <!-- Animation. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
<!--         <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        
<!--         <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p> -->
        
        <!--/ Re-rendering.

      </div>
    </div>
    <!--/ Animation. -->

  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{he2024fluorescencediffractiontomographyusing,
      title={Fluorescence Diffraction Tomography using Explicit Neural Fields}, 
      author={Renzhi He and Yucheng Li and Junjie Chen and Yi Xue},
      year={2024},
      eprint={2407.16657},
      archivePrefix={arXiv},
      primaryClass={physics.optics},
      url={https://arxiv.org/abs/2407.16657}, 
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
