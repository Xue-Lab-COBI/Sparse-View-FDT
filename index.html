<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="NVP is the first technique to enable volumetric reconstruction of label-free biological samples from sparse-view microscopic images, paving the way for real-time 3D imaging of dynamically changing biological samples.">
  <meta name="keywords" content="Diffraction tomography, sparse view reconstruction">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Recover Biological Structure from Sparse-View Diffraction Images with Neural Volumetric Prior</title>

  <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-898QVM6QHF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-898QVM6QHF');
</script>
  
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon2.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
<!--   <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> -->
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  <style>
    .column.content img {
      width: 100%;
      height: auto;
    }
    .caption {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text {
      font-weight: bold;
      margin-top: 10px;
    }
    .caption-text .section-title {
      font-weight: bold;
      font-style: italic;
    }
    .scientific-notation {
      font-family: 'Times New Roman', Times, serif;
      font-style: italic;
    }
    .superscript {
      vertical-align: super;
      font-size: smaller;
    }
  </style>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Recover Biological Structure from Sparse-View Diffraction Images with Neural Volumetric Prior</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://cubhe.mystrikingly.com/">Renzhi He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://hwzhou2020.github.io/">Haowen Zhou</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://yubeichen.com/">Yubei Chen</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://cobi.ucdavis.edu/people">Yi Xue*</a><sup>1</sup>,
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Davis,</span>
            <span class="author-block"><sup>2</sup>California Institute of Technology</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2505.21822"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> 
              <!-- Video Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Xue-Lab-COBI/c_focus_code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->


<!-- <section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-steve">
          <video poster="" id="steve" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/steve.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-chair-tp">
          <video poster="" id="chair-tp" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/chair-tp.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-shiba">
          <video poster="" id="shiba" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/shiba.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-fullbody">
          <video poster="" id="fullbody" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/fullbody.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-blueshirt">
          <video poster="" id="blueshirt" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/blueshirt.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-mask">
          <video poster="" id="mask" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/mask.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-coffee">
          <video poster="" id="coffee" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/coffee.mp4"
                    type="video/mp4">
          </video>
        </div>
        <div class="item item-toby">
          <video poster="" id="toby" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/toby2.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->


<section class="section">



  
  <div class="container is-max-desktop">
    
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Volumetric reconstruction of label-free living cells from non-destructive optical microscopic images reveals cellular metabolism in native environments. However, current optical tomography techniques require hundreds of 2D images to reconstruct a 3D volume, hindering them from intravital imaging of biological samples undergoing rapid dynamics. This poses the challenge of reconstructing the entire volume of semi-transparent biological samples from sparse views due to the restricted viewing angles of microscopes and the limited number of measurements. 
            In this work, we develop Neural Volumetric Prior (NVP) for high-fidelity volumetric reconstruction of semi-transparent biological samples from sparse-view microscopic images. NVP integrates explicit and implicit neural representations and incorporates the physical prior of diffractive optics. We validate NVP on both simulated data and experimentally captured microscopic images. Compared to previous methods, NVP significantly reduces the required number of images by nearly 50-fold and processing time by 3-fold while maintaining state-of-the-art performance.
            NVP is the first technique to enable volumetric reconstruction of label-free biological samples from sparse-view microscopic images, paving the way for real-time 3D imaging of dynamically changing biological samples.          </p>
        </div>
      </div>
    </div>
    <!-- Paper video. -->
<!--     <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
  <div class="columns is-centered">
      <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Overview</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/overview.jpg" alt="Matting Example" style="width: 100%; height: auto;">
            <p class="caption">
              <strong>Overview of our method for 3D reconstruction.</strong>
            </p>
            <p>
              <span class="caption">a</span>, Neural volumetric prior (NVP): Predefined 3D grids are reshaped and processed by MLPs to generate the predicted 3D RI volume $\hat{n}$.
              <span class="caption">b</span>, Multi-slice rendering equation: The multi-slice model calculates light propagation through the volumetric sample from the fluorescence sources (white spots at the bottom) by accounting for light diffraction at each slice. Each illumination configuration of the fluorescence sources (e.g., $F_1, F_i, F_n$) interacts with the volume to produce a corresponding set of rendered images ($\hat{I}_1, \hat{I}_i, \hat{I}_n$). The illumination configurations are jointly optimized with the RI volume.
              <span class="caption">c</span>, Loss functions: The predicted images $\hat{I}_{\text{pred}}$ are aligned with ground truth images $I_{\text{GT}}$ and masked based on light coherence, resulting in $\hat{I}_{\text{mask}}$ and $I_{\text{mask}}$. Loss functions, including $L1$, $L2$, and SSIM, are calculated over the masked regions of $\hat{I}_{\text{pred}}$ and $I_{\text{GT}}$, and a total variation (TV) regularizer $R(\hat{n})$ is applied. 
            </p>

          </div>
        </div>
      </div>
            <!-- Matting. -->
      <div class="column">
        <h2 class="title is-3">Representation Methods</h2>
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/nvp.jpg" alt="Matting Example" style="width: 100%; height: auto;">
            <p class="caption">
              <strong>Illustrations of different neural representation methods.</strong>
            </p>
            <p>
              <span class="caption">a</span>, Explicit neural representation: A non-parametric method in which the refractive index (RI) distribution $\hat{n}$ is directly reconstructed by the projection function $W_{\text{enr}}(x, y, z)$, which provide a one-to-one mapping from spatial coordinates to RI values \cite{Xue:22, FDT}.
              <span class="caption">b</span>,Implicit neural representation: Instead of directly reconstructing $\hat{n}$, this method optimizes the parameters of a  multi-layer perceptron (MLP) $F_{\text{inr}}$, which is then used to predict $\hat{n}$
              <span class="caption">c</span>, Triplane: A hybrid method combining non-parametric and parametric components by solving both the triplane features ${W_{xy}, W_{xz}, W_{yz}}$ and the neural model $F_{\text{tri}}$ to reconstruct $\hat{n}$ 
              <span class="caption">d</span>, NVP: Our proposed hybrid approach reconstructs $\hat{n}$ by integrating an uncompressed volumetric prior ${W_{xyz}}$ into the neural model $F_{\text{nvp}}$.

          </div>
        </div>
      </div>

    
      <!-- <style>
        .column.content img {
          width: 100%;
          height: auto;
        }
      </style> -->
    </div>

    <!--/ Matting. -->



<div class="columns is-centered">
  <div class="column is-full-width">
    <h2 class="title is-3">Results</h2>

    <div class="content has-text-justified">
<!--       <p>
        We verify the validation of our model as well as each component in the
        model under the simulation 'ucdavis' data. Then, we applied the FDT to experimental
        data of thin (MDCK) and thick (3D muscle tube) sample to show the effectiveness of our model for variety of
        structure and z-section ability. 
      </p> -->
      <div class="related-work">
        <!-- <h3 class="title is-4">Fluorescent Beads through a Mouse Skull</h3>
        <div class="work-item">
          <a href="">
            <img src="./static/images/Figure2.jpg" alt="Progressive Encoding for Neural Optimization">
          </a>
          <p class="caption">
            <strong>Overview of our method for 3D reconstruction.</strong>
          </p>
          <p>
            <span class="caption">a</span>, Neural volumetric prior (NVP): Predefined 3D grids are reshaped and processed by MLPs to generate the predicted 3D RI volume $\hat{n}$.
            <span class="caption">b</span>, Multi-slice rendering equation: The multi-slice model calculates light propagation through the volumetric sample from the fluorescence sources (white spots at the bottom) by accounting for light diffraction at each slice. Each illumination configuration of the fluorescence sources (e.g., $F_1, F_i, F_n$) interacts with the volume to produce a corresponding set of rendered images ($\hat{I}_1, \hat{I}_i, \hat{I}_n$). The illumination configurations are jointly optimized with the RI volume.
            <span class="caption">c</span>, Loss functions: The predicted images $\hat{I}_{\text{pred}}$ are aligned with ground truth images $I_{\text{GT}}$ and masked based on light coherence, resulting in $\hat{I}_{\text{mask}}$ and $I_{\text{mask}}$. Loss functions, including $L1$, $L2$, and SSIM, are calculated over the masked regions of $\hat{I}_{\text{pred}}$ and $I_{\text{GT}}$, and a total variation (TV) regularizer $R(\hat{n})$ is applied. 
          </p>
        </div> -->
        
        
        <div class="work-item">
          <h3 class="title is-4">Synthetic Cells</h3>
          <a href="">
            <img src="./static/images/beads.jpg" alt="D-NeRF">
          </a>
          <p class="caption">
            <strong>NVP reconstructs the 3D RI of the synthetic cell dataset with micron-scale resolution and excellent optical sectioning.</strong>
          </p>
          <p>
            <span class="caption">a</span>, Reconstructed 3D RI distribution showing the spatial distribution of cells within a 154 um * 154 um * 24 um volume. 
            <span class="caption">b</span>, Predicted fluorescence images under the illumination from two representative fluorescence sources, demonstrating the accuracy of the reconstructed model compared to the ground-truth data.
            <span class="caption">c</span>, Cross-sectional comparison of RI reconstructed by NVP (top) and ground-truth (bottom) along the axial direction, illustrating consistency between the reconstructed and ground-truth RI distribution across the sample volume. Color bars indicate intensity ranges for predicted images, absolute errors, and RI values.
        </div>
        
<!--         <div class="columns is-centered"> -->


        <p>
        </p>
        <p>
        </p>
          
        <div class="work-item">
          <h3 class="title is-4">Synthetic Cells Comparison </h3>
          <a href="">
            <img src="./static/images/beads2.jpg" alt="Blood Vessels">
          </a>
          <p class="caption">
            <strong>NVP achieves more accurate reconstruction compared to the explicit and triplane methods.</strong>
          </p>
          <p>
            The top row shows the RI maps produced by each method, with the ground-truth RI map included for reference. The bottom row displays the corresponding error maps, highlighting the reconstruction errors in each method. NVP demonstrates superior performance, producing smoother reconstructions and lower errors that closely match the ground truth. 
          </p>
        </div>

        <p>
        </p>
        <div class="work-item">
          <h3 class="title is-4">Synthetic Tissue</h3>
          <a href="">
            <img src="./static/images/neuron.jpg" alt="NR-NeRF">
          </a>
          <p class="caption">
            <strong>NVP reconstructs the RI of varying morphological structures in the synthetic tissue dataset and outperforms the explicit and triplane methods.</strong>
          </p>
          <p>
            The synthetic tissue contains both continuous structures (blood vessels, red) and sparse structures (neurons, blue). Left: the 3D view of the reconstructed RI. Right: a representative $Z$-slice and zoomed-in views of the reconstructed RI. Scale bars denote 50 um.
  
          /p>
        </div>  

        <div class="work-item">
          <h3 class="title is-4">Living MDCK Cells</h3>
          <a href="">
            <img src="./static/images/MDCK.jpg" alt="NR-NeRF">
          </a>
          <p class="caption">
            <strong>NVP reconstructs the 3D RI of living MDCK cells with superior accuracy and fewer artifacts compared to the results from the explicit and triplane methods.</strong>
          </p>
          <p>
            RI maps (top row), predicted images (bottom row, left), and error maps (bottom row, right) are shown for the explicit method, the triplane method, and NVP. The zoomed-in views (purple and green boxes) of the RI maps are shown in the top row, right. Scale bars denote 20 um.
          /p>
        </div>  
        
      </div>
      
      </div>
    </div>
  </div>
</div>

<style>
  .related-work {
    display: flex;
    flex-direction: column;
    align-items: center;
  }
  .work-item {
    text-align: center;
    margin-bottom: 20px;
    width: 100%;
  }
  .work-item img {
    display: block;
    margin: 0 auto;
    width: 100%;
    height: auto;
  }
  .caption {
    font-weight: bold;
    margin-top: 10px;
  }
</style>
</div>

<div class="container is-max-desktop">

    <div class="columns is-centered">

      <!-- Visual Effects. -->
      
    
    <!-- Concurrent Work. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Links</h2>

        <div class="content has-text-justified">
          <p>
            There's a lot of excellent work that was introduced around the same time as ours.
          </p>
          <p>
            <a href="https://arxiv.org/abs/2104.09125">Progressive Encoding for Neural Optimization</a> introduces an idea similar to our windowed position encoding for coarse-to-fine optimization.
          </p>
          <p>
            <a href="https://www.albertpumarola.com/research/D-NeRF/index.html">D-NeRF</a> and <a href="https://gvv.mpi-inf.mpg.de/projects/nonrigid_nerf/">NR-NeRF</a>
            both use deformation fields to model non-rigid scenes.
          </p>
          <p>
            Some works model videos with a NeRF by directly modulating the density, such as <a href="https://video-nerf.github.io/">Video-NeRF</a>, <a href="https://www.cs.cornell.edu/~zl548/NSFF/">NSFF</a>, and <a href="https://neural-3d-video.github.io/">DyNeRF</a>
          </p>
          <p>
            There are probably many more by the time you are reading this. Check out <a href="https://dellaert.github.io/NeRF/">Frank Dellart's survey on recent NeRF papers</a>, and <a href="https://github.com/yenchenlin/awesome-NeRF">Yen-Chen Lin's curated list of NeRF papers</a>.
          </p>
        </div>
      </div>
    </div> -->
    <!--/ Concurrent Work. -->

  </div>
</section>

    <!-- Animation. -->
<!--     <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Animation</h2> -->

        <!-- Interpolating. -->
<!--         <h3 class="title is-4">Interpolating states</h3>
        <div class="content has-text-justified">
          <p>
            We can also animate the scene by interpolating the deformation latent codes of two input
            frames. Use the slider here to linearly interpolate between the left frame and the right
            frame.
          </p>
        </div>
        <div class="columns is-vcentered interpolation-panel">
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_start.jpg"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Start Frame</p>
          </div>
          <div class="column interpolation-video-column">
            <div id="interpolation-image-wrapper">
              Loading...
            </div>
            <input class="slider is-fullwidth is-large is-info"
                   id="interpolation-slider"
                   step="1" min="0" max="100" value="0" type="range">
          </div>
          <div class="column is-3 has-text-centered">
            <img src="./static/images/interpolate_end.jpg"
                 class="interpolation-image"
                 alt="Interpolation end reference image."/>
            <p class="is-bold">End Frame</p>
          </div>
        </div>
        <br/> -->
        <!--/ Interpolating. -->

        <!-- Re-rendering. -->
        
<!--         <h3 class="title is-4">Re-rendering the input video</h3>
        <div class="content has-text-justified">
          <p>
            Using <span class="dnerf">Nerfies</span>, you can re-render a video from a novel
            viewpoint such as a stabilized camera by playing back the training deformations.
          </p> -->
        
        <!--/ Re-rendering.

      </div>
    </div>
    <!--/ Animation. -->

  
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>

      @misc{he2025compressivefourierdomainintensitycoupling,
      title={Compressive Fourier-Domain Intensity Coupling (C-FOCUS) enables near-millimeter deep imaging in the intact mouse brain in vivo}, 
      author={Renzhi He and Yucheng Li and Brianna Urbina and Jiandi Wan and Yi Xue},
      year={2025},
      eprint={2505.21822},
      archivePrefix={arXiv},
      primaryClass={physics.optics},
      url={https://arxiv.org/abs/2505.21822},} 
      </code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
